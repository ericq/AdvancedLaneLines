{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Make this mode to False, so it will only process static images with verbose image output\n",
    "# if true, the static images will be processed still, but no interim images drawn\n",
    "# when submit this work, it should be set to True\n",
    "VIDEO_MODE = True\n",
    "\n",
    "# plot input vs output side by side\n",
    "# note that imshow expects rgb img\n",
    "\n",
    "def plot_two_imgs(left_img, right_img, left_cmap=None, right_cmap=None, left_title = \"left img\", right_title= \"right img\", ):\n",
    "    if VIDEO_MODE:\n",
    "        return\n",
    "    \n",
    "    # Plot the result\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(left_img, cmap=left_cmap)\n",
    "    ax1.set_title(left_title, fontsize=50)\n",
    "    ax2.imshow(right_img, cmap=right_cmap)\n",
    "    ax2.set_title(right_title, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, I'll compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calibrate camera using collection of chessboard pictures. \n",
    "# Note: \n",
    "# 1. calibration will not happen if found coners less than half of the checkboard images \n",
    "# 2. Same image size for the correlation image and future image to be undistorted\n",
    "# images_path: input file name patterns, default value:camera_cal/calibration*.jpg\n",
    "# corner_num_x: number of corners along the x axis, default value:9\n",
    "# corner_num_y: number of corners along the y axis, default value:6\n",
    "# output_path:  output the corner found images for debugging purpose. default value:\"camera_cal_output\"\n",
    "# img_size: this corresponds to the picture to be undistorted. default value: None, use the calibration img size\n",
    "#\n",
    "# The size of the image, which is passed into the calibrateCamera function, is just the height and width of the image.\n",
    "# One way to retrieve these values is by retrieving them from the grayscale image shape array gray.shape[::-1]. \n",
    "# This returns the image width and height in pixel values like (1280, 960).\n",
    "# Another way to retrieve the image shape, is to get them directly from the color image by retrieving the first two \n",
    "# values in the color image shape array using img.shape[1::-1]. This code snippet asks for just the first two values \n",
    "# in the shape array, and reverses them. Note that in our case we are working with a greyscale image, \n",
    "# so we only have 2 dimensions (color images have three, height, width, and depth), so this is not necessary.\n",
    "\n",
    "# Return value - same as cv2.calibrateCamera return\n",
    "# plus result saved to <output_path>/cam_calibration_result_pickle.p for future use\n",
    " \n",
    "\n",
    "def calibrate_camera (images_path=\"camera_cal/calibration*.jpg\", corner_num_x=9, corner_num_y=6, output_path=\"camera_cal_output/\", img_size=None):\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((corner_num_y*corner_num_x,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:corner_num_x, 0:corner_num_y].T.reshape(-1,2)\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(images_path)\n",
    "\n",
    "\n",
    "    objpoints = [] # 3D points in real world space\n",
    "    imgpoints = [] # 2D points in img plane\n",
    "\n",
    "    # prepare output file dir\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    corners_found = 0\n",
    "    for idx, fname in enumerate(images):\n",
    "        img = mpimg.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (corner_num_x,corner_num_y), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            corners_found += 1\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            cv2.drawChessboardCorners(img, (corner_num_x,corner_num_y), corners, ret)\n",
    "            write_name = output_path+'corners_found_in_'+os.path.basename(fname)\n",
    "            mpimg.imsave(write_name, img)\n",
    "            #print (\"corners found and saved as:\" + write_name)\n",
    "\n",
    "        else:\n",
    "            print (\"coners not found in: \" + fname +\" skip it\")\n",
    "       \n",
    "    \n",
    "    if corners_found > idx / 2:\n",
    "        print (\"OK: found cornders in more than half of the input images, carry on with calibration\")\n",
    "        if img_size is None:\n",
    "            img_size = (img.shape[1], img.shape[0])\n",
    "            print(\"img_size is None, use calibration img size: \" + str(img_size))\n",
    "            \n",
    "            \n",
    "        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "        \n",
    "        # Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "        dist_pickle = {}\n",
    "        dist_pickle[\"mtx\"] = mtx\n",
    "        dist_pickle[\"dist\"] = dist\n",
    "        cam_calibration_result = output_path+\"cam_calibration_result_pickle.p\"\n",
    "        pickle.dump( dist_pickle, open( cam_calibration_result, \"wb\" ) )\n",
    "        \n",
    "        print(\"camera calibration result saved in file \" + cam_calibration_result + \" for future use\" )\n",
    "    \n",
    "        return ret, mtx, dist, rvecs, tvecs\n",
    "        \n",
    "    else:\n",
    "        print (\"Error: found cornders in less than half of the input images, stopped calibration.\")\n",
    "        return -1, None, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Note of Applying a distortion correction to raw images.\n",
    "No wrapping code needed. please use cv2.undistort directly in the future pipeline.\n",
    "Example code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved camera matrix and distortion coefficients from the pickle dump file\n",
    "# dist_pickle = pickle.load( open( \"wide_dist_pickle.p\", \"rb\" ) )\n",
    "# mtx = dist_pickle[\"mtx\"]\n",
    "# dist = dist_pickle[\"dist\"]\n",
    "# Use the OpenCV undistort() function to remove distortion\n",
    "# undist = cv2.undistort(img, mtx, dist, None, mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some tools before proceed, including Gaussian blur and use region of interest so we can skip other noise may occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    `vertices` should be a numpy array of integer points.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, Use color transforms, gradients, etc., to create a thresholded binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes an image, gradient orientation,\n",
    "# and threshold min / max values.\n",
    "\n",
    "# the gray image color space has bad result with shadows, try \n",
    "\n",
    "def abs_sobel_thresh(gray_img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    \n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return grad_binary\n",
    "\n",
    "\n",
    "# Define a function to threshold an image for a given range and Sobel kernel\n",
    "def dir_threshold(gray_img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    dir_binary =  np.zeros_like(absgraddir)\n",
    "    dir_binary[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return dir_binary\n",
    "\n",
    "# Define a function to return the magnitude of the gradient\n",
    "# for a given sobel kernel size and threshold values\n",
    "def mag_thresh(gray_img, sobel_kernel=3, thresh=(0, 255)):\n",
    "\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    mag_binary = np.zeros_like(gradmag)\n",
    "    mag_binary[(gradmag >= thresh[0]) & (gradmag <= thresh[1])] = 1\n",
    "    \n",
    "\n",
    "    # Return the binary image\n",
    "    return mag_binary\n",
    "\n",
    "# in HLS color space, the the S channel is still doing a fairly robust job of picking up the lines \n",
    "# under very different color and contrast conditions, which helps to  Reliably detect different colors \n",
    "# of lane lines under varying degrees of daylight and shadow.\n",
    "def color_thresh(hls_img, s_thresh=(170, 255)):\n",
    "    s_channel = hls_img[:,:,2]\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    return s_binary\n",
    "\n",
    "# a larger kernel size make it smoother\n",
    "def combined_thresh(rgb_img, ksize=3, grad_thresh=(0, 255), magitude_thresh=(0, 255), dir_thresh=(0, np.pi/2), s_threash=(170,255)):\n",
    "    \n",
    "    # for plotting purpose.\n",
    "    hls_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "\n",
    "    # as input to various threshold \n",
    "    gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Define a kernel size for Gaussian smoothing / blurring\n",
    "    # Kernel Must be an odd number (3, 5, 7...)\n",
    "    blurgray_img = gaussian_blur(gray_img, 3) \n",
    "\n",
    "    \n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(blurgray_img, orient='x', sobel_kernel=ksize, thresh=grad_thresh)    \n",
    "    #plot_two_imgs(left_img=gray_img, left_cmap='gray', right_img=gradx, right_cmap='gray', right_title = \"gradx\" )\n",
    "        \n",
    "    grady = abs_sobel_thresh(blurgray_img, orient='y', sobel_kernel=ksize, thresh=grad_thresh)\n",
    "    #plot_two_imgs(left_img=gray_img, left_cmap='gray', right_img=grady, right_cmap='gray', right_title = \"grady\" )\n",
    "\n",
    "    mag_binary = mag_thresh(blurgray_img, sobel_kernel=ksize, thresh=magitude_thresh)  \n",
    "    #plot_two_imgs(left_img=gray_img, left_cmap='gray', right_img=mag_binary, right_cmap='gray', right_title = \"mag\" )\n",
    "\n",
    "    dir_binary = dir_threshold(blurgray_img, sobel_kernel=ksize, thresh=dir_thresh)\n",
    "    #plot_two_imgs(left_img=gray_img, left_cmap='gray', right_img=dir_binary, right_cmap='gray', right_title = \"directional\" )\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    #plot_two_imgs(left_img=gray_img, left_cmap='gray', right_img=combined, right_cmap='gray', right_title = \"combined\" )\n",
    "\n",
    "\n",
    "    # apply color\n",
    "    s_binary = color_thresh(hls_img, s_threash)\n",
    "    #plot_two_imgs(left_img=hls_img, right_img=s_binary, right_title = \"combined\" )\n",
    "    \n",
    "    # Stack each channel to view their individual contributions in green and blue respectively\n",
    "    # This returns a stack of the two binary images, whose components you can see as different colors\n",
    "    # stacked_color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary)) * 255\n",
    "\n",
    "    # Combine the two binary thresholds\n",
    "    combined_with_color = np.zeros_like(combined)\n",
    "    combined_with_color[(s_binary == 1) | (combined == 1)] = 1\n",
    "    plot_two_imgs(left_img=rgb_img, right_img=combined_with_color, right_cmap='gray', right_title = \"combined\" )\n",
    "\n",
    "\n",
    "    return combined_with_color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform perspective \n",
    "Use one of the test image for perspective transform and then use it for other images assuming the perspective didn't change in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"straight_lines1.jpg\" as the sample\n",
    "# example of using this matrix to undistort:\n",
    "# img_size = (undistorted.shape[1], undistorted.shape[0])\n",
    "# warped = cv2.warpPerspective(undistorted, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "def get_perspective_transform_matrix():\n",
    "     \n",
    "    # a) define 4 source points src = np.float32([[,],[,],[,],[,]])\n",
    "    \n",
    "    # Note, need to box in areas that you want to draw lane-zone in the future\n",
    "    # this is to ensure we included enough lane segments\n",
    "    # src = np.float32([[824,540],[1042,678],[262,678],[464,540]])\n",
    "    #src = np.float32([[734,480],[1042,678],[262,678],[556,480]])\n",
    "    src = np.float32([[748,490],[1050,678],[262,678],[540,490]])\n",
    "\n",
    "\n",
    "\n",
    "    #Note: you could pick any four of the detected corners \n",
    "    # as long as those four corners define a rectangle\n",
    "    #One especially smart way to do this would be to use four well-chosen\n",
    "    # corners that were automatically detected during the undistortion steps\n",
    "    #We recommend using the automatic detection of corners in your code\n",
    "    \n",
    "    # b) define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "    #dst = np.float32([[1042,120],[1042,719],[262,719],[282,120]])\n",
    "    dst = np.float32([[1042,5],[1042,719],[262,719],[282,5]])\n",
    "\n",
    "    \n",
    "    # above dest box is ok but not great, because it zoomed into a small area of the map only\n",
    "    # zoom out using below ratio - no change to box center \n",
    "    #zoom_by = 0.3\n",
    "    #dx = (dst[0][0]-dst[3][0])*zoom_by\n",
    "    #dy = (dst[2][1] - dst[0][1])*zoom_by\n",
    "\n",
    "    #new_dst = np.zeros_like(dst)\n",
    "    # top-right, bottom-right, bottom-left, top-left\n",
    "    #new_dst = [ [dst[0][0]-dx , dst[0][1]+dy], \n",
    "    #            [dst[1][0]-dx , dst[1][1]-dy], \n",
    "    #           [dst[2][0]+dx , dst[2][1]-dy],\n",
    "    #            [dst[3][0]+dx , dst[3][1]+dy] ]\n",
    "\n",
    "    # c) use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    INVM = cv2.getPerspectiveTransform(dst, src)\n",
    "                       \n",
    "            \n",
    "    # Return the resulting image and matrix\n",
    "    return M, INVM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the histogram for the two lines\n",
    "The first step we'll take is to split the histogram into two sides, one for each lane line.\n",
    "\n",
    "Set up windows and window hyperparameters\n",
    "Our next step is to set a few hyperparameters related to our sliding windows, and set them up to iterate across the binary activations in the image. We have some base hyperparameters below, but don't forget to try out different values in your own implementation to see what works best!\n",
    "\n",
    "Iterate through nwindows to track curvature\n",
    "Now that we've set up what the windows look like and have a starting point, we'll want to loop for nwindows, with the given window sliding left or right if it finds the mean position of activated pixels within the window to have shifted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lane_pixels(binary_warped):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "        (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "        (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window #\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    return leftx, lefty, rightx, righty, out_img\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Fit a polynomial\n",
    "Now that we have found all our pixels belonging to each line through the sliding window method, it's time to fit a polynomial to the line. First, we have a couple small steps to ready our pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = []  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float')\n",
    "        # total number of matched pixels\n",
    "        self.px_count = 0\n",
    "        \n",
    "        # hypger parameter for curvature and center position evaluation\n",
    "        self.ym_per_pix = None  \n",
    "        self.xm_per_pix = None \n",
    "\n",
    "    #Note, all_x and all_y stands for the pixels matches the lane,\n",
    "    # but we don't store the raw info in order to make the Line object lean. \n",
    "    # instead, we derive all the needed information from here, e.g. count, curvature, centerpos, min-y, max-y\n",
    "    \n",
    "    def add_fit(self, fit, all_x, all_y):\n",
    "        # add a found fit to the line, up to n\n",
    "        if fit is not None:\n",
    "            if self.best_fit is not None:\n",
    "                # if we have a best fit, see how this new fit compares\n",
    "                self.diffs = abs(fit-self.best_fit)\n",
    "            if (self.diffs[0] > 0.001 or \\\n",
    "               self.diffs[1] > 1.0 or \\\n",
    "               self.diffs[2] > 100.) and \\\n",
    "               len(self.current_fit) > 0:\n",
    "                self.detected = False\n",
    "            else:\n",
    "                self.detected = True\n",
    "                self.px_count = np.count_nonzero(all_x)\n",
    "                self.current_fit.append(fit)\n",
    "                if len(self.current_fit) > 50:\n",
    "                    # throw out old fits, keep newest n\n",
    "                    self.current_fit = self.current_fit[len(self.current_fit)-50:]\n",
    "                self.best_fit = np.average(self.current_fit, axis=0)\n",
    "                \n",
    "                # update the cuvrate etc. based on the best fit\n",
    "                self.radius_of_curvature = self.measure_curvature(all_x, all_y)\n",
    "                \n",
    "        # or remove one from the history, if not found\n",
    "        else:\n",
    "            self.detected = False\n",
    "            if len(self.current_fit) > 0:\n",
    "                # throw out oldest fit\n",
    "                self.current_fit = self.current_fit[:len(self.current_fit)-1]\n",
    "            if len(self.current_fit) > 0:\n",
    "                # if there are still any fits in the queue, best_fit is their average\n",
    "                self.best_fit = np.average(self.current_fit, axis=0)\n",
    "                \n",
    "    # input:\n",
    "    # img - binary img \n",
    "    # \n",
    "\n",
    "    def measure_curvature(self, allx, ally):\n",
    "        '''\n",
    "        Calculates the curvature of polynomial functions in meters.\n",
    "        '''\n",
    "\n",
    "        # Define y-value where we want radius of curvature\n",
    "        # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "        y_eval = np.max(ally)\n",
    "\n",
    "        # Fit a second order polynomial to pixel positions in each lane line\n",
    "        # Fit new polynomials to x,y in world space\n",
    "        fit_cr = np.polyfit(ally*self.ym_per_pix, allx*self.xm_per_pix, 2)\n",
    "\n",
    "        # Calculation of R_curve (radius of curvature)\n",
    "        curverad = ((1 + (2*fit_cr[0]*y_eval*self.ym_per_pix + fit_cr[1])**2)**1.5) / np.absolute(2*fit_cr[0])\n",
    "        \n",
    "        return curverad\n",
    "\n",
    "\n",
    "\n",
    "# Define a class to store both left and right line\n",
    "class Lane():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # hypger parameter for curvature and center position evaluation\n",
    "        self.ym = 30 # meters per pixel in y dimension, (weakly) assuming 30m length \n",
    "        self.xm = 3.7 # meters per pixel in x dimension, (strongly) assuming 3.7m width \n",
    "        self.img_shape = (720, 1280) # used to calcuate center position.\n",
    "        self.warped_shape = (720, 1280) # used to represent the size of the bird eye view shape\n",
    "\n",
    "        # each lane musg have a left line and a right line\n",
    "        self.left_line = Line()\n",
    "        self.left_line.ym_per_pix = self.ym / self.img_shape[0] \n",
    "        self.left_line.xm_per_pix = self.xm / self.img_shape[1] \n",
    "        \n",
    "        self.right_line = Line()\n",
    "        self.right_line.ym_per_pix = self.ym / self.img_shape[0] \n",
    "        self.right_line.xm_per_pix = self.xm / self.img_shape[1]\n",
    "\n",
    "\n",
    "    #assume the camera is mounted at the center of the car and the deviation of the midpoint of the lane \n",
    "    # from the center of the image is the offset.\n",
    "    def get_center_position(self):\n",
    "        mypos = self.img_shape[1]/2\n",
    "        \n",
    "        h = self.img_shape[0]\n",
    "        \n",
    "        left_fit = self.left_line.best_fit\n",
    "        right_fit = self.right_line.best_fit\n",
    "\n",
    "        lfit = left_fit[0]*h**2 + left_fit[1]*h + left_fit[2]\n",
    "        rfit = right_fit[0]*h**2 + right_fit[1]*h + right_fit[2]\n",
    "        lpos = (rfit + lfit) /2\n",
    "        center_dist = (mypos - lpos) * (self.xm / self.img_shape[1])\n",
    "        \n",
    "        return center_dist\n",
    "    \n",
    "    # lane zone shape is determined by the polyfit result. \n",
    "    # original_img is the orginal image as the backdrop\n",
    "    # warped_img - is the one used to do polyfitting. used to determine shape info\n",
    "    # Minv - perspective transform matrix , from warped bird eye view to unwarped camera view\n",
    "    # left_line - detected result (note this is not orignal image pixel, but calcuated per the polyfit\n",
    "    # right_line - deteed result\n",
    "    \n",
    "    def draw(self, original_img, Minv):\n",
    "        # Create an image to draw the lines on\n",
    "        warp_zero = np.zeros(self.warped_shape).astype(np.uint8)\n",
    "        color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "        \n",
    "        # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "        \n",
    "        ## Generate x and y values for plotting\n",
    "        ploty = np.linspace(0, self.warped_shape[0]-1, self.warped_shape[0] )\n",
    "        \n",
    "        left_fit = self.left_line.best_fit\n",
    "        right_fit = self.right_line.best_fit\n",
    "        \n",
    "        try:\n",
    "            left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "            right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        except TypeError:\n",
    "            # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "            print('The function failed to fit a line!')\n",
    "            left_fitx = 1*ploty**2 + 1*ploty\n",
    "            right_fitx = 1*ploty**2 + 1*ploty\n",
    "        \n",
    "        pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "        \n",
    "        # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "        newwarp = cv2.warpPerspective(color_warp, Minv, (original_img.shape[1], original_img.shape[0])) \n",
    "        \n",
    "            \n",
    "        # Combine the result with the original image\n",
    "        result = cv2.addWeighted(original_img, 1, newwarp, 0.3, 0)\n",
    "\n",
    "\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_warped - input binary image in warped bird-eye view\n",
    "# output:\n",
    "# ouptut_img - \n",
    "# left_line, right line in Line class\n",
    "\n",
    "def fit_polynomial(binary_warped):\n",
    "    \n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, out_img = find_lane_pixels(binary_warped)\n",
    "\n",
    "    # Fit a second order polynomial to each using `np.polyfit`\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    try:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1*ploty**2 + 1*ploty\n",
    "        right_fitx = 1*ploty**2 + 1*ploty\n",
    "\n",
    "    ## Visualization ##\n",
    "    # Colors in the left and right lane regions\n",
    "    out_img[lefty, leftx] = [255, 0, 0]\n",
    "    out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    # Plots the left and right polynomials on the lane lines\n",
    "    #plt.plot(left_fitx, ploty, color='yellow')\n",
    "    #plt.plot(right_fitx, ploty, color='yellow')\n",
    "    \n",
    "    # prepare the result\n",
    "    lane = Lane()\n",
    "    \n",
    "    lane.left_line.add_fit(left_fit, leftx, lefty)\n",
    "    lane.right_line.add_fit(right_fit, rightx, righty)\n",
    "\n",
    "\n",
    "    return out_img, lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline linking all processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect lane from a image file\n",
    "#\n",
    "\n",
    "\n",
    "def adv_lane_detection_pipeline(rgb_img, calibration_mtx, calibration_dist):\n",
    "\n",
    "    img_size = (rgb_img.shape[1], rgb_img.shape[0])\n",
    "    \n",
    "    # Apply a distortion correction to raw imageg\n",
    "    undist = cv2.undistort(rgb_img, calibration_mtx, calibration_dist, None, calibration_mtx)\n",
    "    \n",
    "\n",
    "    # Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "    # experiment about the grad and magnitude threshold:\n",
    "    # lower the L-threshold, more noise. should not be lower than 30, 50 looks good.\n",
    "    # higher the H-threshold, brighter the line. nothing wrong with it. use 200. \n",
    "    \n",
    "    thresholded = combined_thresh(undist, 15, (50,200),(50,200),(0.7,1.3),(170,255))\n",
    "    \n",
    "    imshape = thresholded.shape\n",
    "    \n",
    "    #a typical lane zone - straightline [[748,490],[1050,678],[262,678],[540,490]]\n",
    "    \n",
    "    #vertices = np.array([[(0,imshape[0]),(430, 330), (620, 330), (imshape[1],imshape[0])]], dtype=np.int32)\n",
    "    mgin = 150\n",
    "    vertices = np.array([[(748+mgin,490-mgin),(1050+mgin,678+mgin), (262-mgin,678+mgin), (540-mgin,490-mgin)]], dtype=np.int32)\n",
    "    masked_thresholded = region_of_interest(thresholded, vertices)\n",
    "\n",
    "\n",
    "        \n",
    "    # Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "    M, INVM = get_perspective_transform_matrix()\n",
    "    \n",
    "    img_size = (masked_thresholded.shape[1], masked_thresholded.shape[0])\n",
    "    warped = cv2.warpPerspective(masked_thresholded, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    #plot_two_imgs(left_img=thresholded, left_cmap='gray', right_img=warped, right_cmap='gray', right_title = \"warped\" )\n",
    "    \n",
    "    \n",
    "    # Detect lane pixels and fit to find the lane boundary.    \n",
    "    lane_line_img, lane = fit_polynomial (warped) \n",
    "    plot_two_imgs(left_img=rgb_img, right_img=lane_line_img, right_title = \"warped\" )\n",
    "    \n",
    "    left_curv = lane.left_line.radius_of_curvature\n",
    "    right_curv = lane.right_line.radius_of_curvature\n",
    "    \n",
    "    cpos = lane.get_center_position()\n",
    "    \n",
    "   \n",
    "    # Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "    result = lane.draw(rgb_img, INVM)\n",
    "\n",
    "    text1 = 'Curveature: ' + '{:04.1f}'.format(left_curv) + 'm'\n",
    "    cv2.putText(result, text1, (60,50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (102,0,51), 2, cv2.LINE_AA)\n",
    "\n",
    "    text2 = 'Center Offset: ' + '{:04.2f}'.format(cpos) + 'm' \n",
    "    cv2.putText(result, text2, (60,90), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (102,0,51), 2, cv2.LINE_AA)\n",
    "\n",
    "    plot_two_imgs(left_img=rgb_img, right_img=result,left_title = \"original\", right_title = \"lane highlighted\" )\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coners not found in: camera_cal/calibration5.jpg skip it\n",
      "coners not found in: camera_cal/calibration4.jpg skip it\n",
      "coners not found in: camera_cal/calibration1.jpg skip it\n",
      "OK: found cornders in more than half of the input images, carry on with calibration\n",
      "img_size is None, use calibration img size: (1280, 720)\n",
      "camera calibration result saved in file camera_cal_output/cam_calibration_result_pickle.p for future use\n",
      "Processed img test6.jpg\n",
      "Processed img test5.jpg\n",
      "Processed img test4.jpg\n",
      "Processed img test1.jpg\n",
      "Processed img test3.jpg\n",
      "Processed img test2.jpg\n",
      "Processed img straight_lines2.jpg\n",
      "Processed img straight_lines1.jpg\n"
     ]
    }
   ],
   "source": [
    "# Calibrate camera\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = calibrate_camera()\n",
    "\n",
    "if ret < 0:\n",
    "    print (\"Error: calibration camera failed.\")\n",
    "    exit \n",
    "\n",
    "# iterate through every input file\n",
    "INPUT_DIR = \"test_images/\"\n",
    "OUTPUT_DIR = \"test_images_output/\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(INPUT_DIR):  \n",
    "    rgb_img = mpimg.imread(INPUT_DIR+filename)\n",
    "    result_img = adv_lane_detection_pipeline (rgb_img, mtx, dist)\n",
    "    mpimg.imsave(OUTPUT_DIR+filename, result_img)\n",
    "    print(\"Processed img \" + filename )\n",
    "    \n",
    "    # debug on, please remove below line, only do one img\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frame(input_img):\n",
    "        \n",
    "    output_img = adv_lane_detection_pipeline (input_img, mtx, dist)\n",
    "    \n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/project_video.mp4\n",
      "[MoviePy] Writing video test_videos_output/project_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [04:28<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/project_video.mp4 \n",
      "\n",
      "CPU times: user 17min 6s, sys: 1min 5s, total: 18min 12s\n",
      "Wall time: 4min 29s\n",
      "[MoviePy] >>>> Building video test_videos_output/challenge_video.mp4\n",
      "[MoviePy] Writing video test_videos_output/challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 134/485 [00:27<01:05,  5.39it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-175>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-174>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                            progress_bar=progress_bar)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         for t,frame in clip.iter_frames(progress_bar=progress_bar, with_times=True,\n\u001b[0;32m--> 218\u001b[0;31m                                         fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-134>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dd520391db57>\u001b[0m in \u001b[0;36mprocess_video_frame\u001b[0;34m(input_img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_video_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_lane_detection_pipeline\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f45d637b94c7>\u001b[0m in \u001b[0;36madv_lane_detection_pipeline\u001b[0;34m(rgb_img, calibration_mtx, calibration_dist)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Detect lane pixels and fit to find the lane boundary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlane_line_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_polynomial\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwarped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mplot_two_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlane_line_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"warped\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-504088d3ad3e>\u001b[0m in \u001b[0;36mfit_polynomial\u001b[0;34m(binary_warped)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Fit a second order polynomial to each using `np.polyfit`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mleft_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlefty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mright_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrighty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 1D vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected non-empty vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 1D or 2D array for y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    }
   ],
   "source": [
    "def process_video_clip(input_video_name):\n",
    "\n",
    "    video_input_dir = './'\n",
    "    video_output_dir = 'test_videos_output/'\n",
    "    os.makedirs(os.path.dirname(video_output_dir), exist_ok=True)\n",
    "\n",
    "    clip1 = VideoFileClip(video_input_dir+input_video_name)\n",
    "    lane_clip1 = clip1.fl_image(process_video_frame) #NOTE: this function expects color images!!\n",
    "    %time lane_clip1.write_videofile(video_output_dir + input_video_name, audio=False)\n",
    "\n",
    "\n",
    "if VIDEO_MODE:\n",
    "    process_video_clip('project_video.mp4')\n",
    "    process_video_clip('challenge_video.mp4')\n",
    "    #process_video_clip('harder_challenge_video.mp4')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
